# Install dependencies if needed: pip install -r ../requirements.txt
import os
from dotenv import load_dotenv
load_dotenv()  # For API keys if using external LLMs; skip for local

import mlflow
import mlflow.langchain

with mlflow.start_run():
    mlflow.langchain.log_model(qa_chain, "rag_model")
    mlflow.log_param("embedding_model", "all-MiniLM-L6-v2")
    mlflow.log_param("llm_model", "gpt2")
    # Run query and log metric
    result = qa_chain({"query": query})
    mlflow.log_metric("response_length", len(result["result"]))

from langchain_community.document_loaders import TextLoader  # Or PyPDFLoader for PDFs
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings  # Open-source embeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFaceHub  # Or LlamaCpp for local LLM

# Step 1: Load and split documents (e.g., add your data files to /data)
loader = TextLoader("data/sample_doc.txt")  # Create a sample .txt file with some text
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

# Step 2: Embed and store in vector DB
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(texts, embeddings)

# Step 3: Set up LLM (use open-source; get free Hugging Face token if needed)
llm = HuggingFaceHub(repo_id="gpt2", model_kwargs={"temperature": 0.7})  # Replace with 'meta-llama/Llama-2-7b-chat-hf' for better results

# Step 4: Create RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# Query example
query = "What is the main topic?"
result = qa_chain({"query": query})
print(result["result"])
